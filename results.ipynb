{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "os.makedirs('latex', exist_ok=True)\n",
    "os.makedirs('sig', exist_ok=True)\n",
    "os.makedirs('measure_latex', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_point_number = re.compile(r'\\d+\\.\\d+')\n",
    "\n",
    "rename_dict = {\n",
    "    'cos1': 'Cos',\n",
    "    'dam': 'Dam',\n",
    "    'dice1': 'Dice',\n",
    "    'jac1': 'Jac',\n",
    "    'lcs': 'LCS',\n",
    "    'lev': 'Lev',\n",
    "    'osa': 'OSA',\n",
    "    'qg1': 'QG1',\n",
    "    'qg2': 'QG2',\n",
    "    'qg3': 'QG3',\n",
    "    'wlev_del': 'Lev$_D$',\n",
    "    'wlev_ins': 'Lev$_I$',\n",
    "    'wlev_sub': 'Lev$_S$'\n",
    "}\n",
    "\n",
    "replace_names = ['& ' + x.replace('_', '\\\\_') + ' ' for x in rename_dict.values()]\n",
    "\n",
    "metrics_list = [\n",
    "    'AP', \n",
    "    'P@1', 'P@5', 'P@10', 'P@20', 'P@50', 'P@100', 'P@500', 'P@1000',\n",
    "    'R@1', 'R@5', 'R@10', 'R@20', 'R@50', 'R@100', 'R@500', 'R@1000'\n",
    "]\n",
    "\n",
    "short_names = (\n",
    "    'cos1', 'dam', 'dice1', 'jac1', 'lcs', 'lev', 'osa', 'qg1', 'qg2', 'qg3', 'wlev_del', 'wlev_ins', 'wlev_sub'\n",
    ")\n",
    "\n",
    "# langs = [\n",
    "#     'all', 'amh', 'arb', 'bul', 'che', 'cop', 'cym', 'deu',\n",
    "#     'ell', 'eng', 'eus', 'fin', 'fra', 'guj', 'heb', 'hin',\n",
    "#     'hrv', 'hun', 'hye', 'ind', 'isl', 'ita', 'jav', 'jpn',\n",
    "#     'kan', 'kat', 'kaz', 'khm', 'kor', 'lao', 'lit', 'mar',\n",
    "#     'mkd', 'mon', 'mri', 'msa', 'mya', 'nld', 'pol', 'por',\n",
    "#     'rus', 'san', 'slv', 'som', 'spa', 'sqi', 'swe', 'tel',\n",
    "#     'tha', 'tur', 'ukr', 'urd', 'vie', 'xho', 'zho', 'zul'\n",
    "# ]\n",
    "\n",
    "quick_langs = sorted(\n",
    "    [\n",
    "        'all', 'amh', 'arb', 'cym', 'ell', 'eng', 'eus',\n",
    "        'fin', 'heb', 'hin', 'hun', 'hye', 'ind', 'jpn', 'kan',\n",
    "        'khm', 'kor', 'lit', 'mon', 'mri', 'rus', 'spa', 'sqi',\n",
    "        'tha', 'tur', 'urd', 'vie', 'zho', 'zul'\n",
    "    ]\n",
    ")\n",
    "\n",
    "def clean(tex):\n",
    "    tex = tex.replace(\n",
    "        '\\\\begin{table}', '\\\\begin{sidewaystable}\\n\\\\centering'\n",
    "    ).replace(\n",
    "        '\\\\end{table}', '\\\\end{sidewaystable}'\n",
    "    ).replace(\n",
    "        'Lev$_D$', '\\\\textbf{Lev$_D$}'\n",
    "    ).replace(\n",
    "        'Lev$_I$', '\\\\textbf{Lev$_I$}'\n",
    "    ).replace(\n",
    "        'Lev$_S$', '\\\\textbf{Lev$_S$}'\n",
    "    ).replace(\n",
    "        '{llllllllllllll}', '{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\\n\\\\hline'\n",
    "    ).replace(\n",
    "        '{lrrrrrrrrrrrrr}', '{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\\n\\\\hline'\n",
    "    ).replace(\n",
    "        '{ll}', '{|c|c|}\\n\\\\hline'\n",
    "    ).replace(\n",
    "        '\\\\\\\\', '\\\\\\\\\\n\\\\hline'\n",
    "    ).replace(\n",
    "        '\\\\toprule\\n', ''\n",
    "    ).replace(\n",
    "        '\\\\midrule\\n', ''\n",
    "    ).replace(\n",
    "        '\\\\bottomrule\\n', ''\n",
    "    )\n",
    "    return tex\n",
    "\n",
    "alpha = 0.05 / (2 * len(quick_langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(lang):\n",
    "    files = glob.glob(f'metrics/{lang}/*.json')\n",
    "    \n",
    "    agg_results = {\n",
    "        'train': {},\n",
    "        'test': {}\n",
    "    }\n",
    "    unagg_results = {\n",
    "        'train': pd.DataFrame(columns=['query_id', 'measure', 'value', 'dist']),\n",
    "        'test': pd.DataFrame(columns=['query_id', 'measure', 'value', 'dist'])\n",
    "    }\n",
    "    \n",
    "    train_path = os.path.join('latex', f'{lang}_train.tex')\n",
    "    test_path = os.path.join('latex', f'{lang}_test.tex')\n",
    "    \n",
    "    for f in files:\n",
    "        file = os.path.basename(f)\n",
    "        file_split = file.split('_')\n",
    "        split = file_split[-6]\n",
    "        if 'wlev' in file:\n",
    "            dist = '_'.join(file_split[:2])\n",
    "        else:\n",
    "            dist = file_split[0]\n",
    "        j = json.load(open(f, 'r', encoding='utf-8'))\n",
    "        agg_results[split][dist] = j\n",
    "        df_dist = pd.read_csv(f.replace('json', 'csv'), sep='\\t')\n",
    "        df_dist['dist'] = dist\n",
    "        unagg_results[split] = pd.concat([unagg_results[split], df_dist])\n",
    "        \n",
    "    train_df = pd.DataFrame(agg_results['train'])\n",
    "    train_df.rename(columns = rename_dict, inplace=True)\n",
    "    train_df = train_df.reindex(sorted(train_df.columns), axis=1)\n",
    "    train_df = train_df.reindex(metrics_list)\n",
    "    \n",
    "    test_df = pd.DataFrame(agg_results['test'])\n",
    "    test_df.rename(columns = rename_dict, inplace=True)\n",
    "    test_df = test_df.reindex(sorted(test_df.columns), axis=1)\n",
    "    test_df = test_df.reindex(metrics_list)\n",
    "    \n",
    "    agg_results = {\n",
    "        'train': train_df,\n",
    "        'test': test_df\n",
    "    }\n",
    "    \n",
    "    if lang == 'all':\n",
    "        train_caption = f'Metrics for the train split of CogNet. AP = mean average precision, P@k = precision with cutoff k, R@k = recall with cutoff k.'\n",
    "        train_caption += ' Key: Cos = cosine, Dam = Damerau-Levenshtein, Dice = Dice-S\\\\o rensen, Jac = Jaccard, LCS = Longest Common Subsequence, Lev = Levenshtein, Lev$_D$ ='\n",
    "        train_caption += ' Levenshtein$_\\\\text{del}$, Lev$_I$ = Levenshtein$_\\\\text{ins}$, Lev$_S$ = Levenshtein$_\\\\text{sub}$, OSA = Optimal String Alignment,'\n",
    "        train_caption += ' QG1-3 = $q$-Grams with $k\\\\in\\\\{1,2,3\\\\}$.'\n",
    "        test_caption = 'Metrics for the test split of CogNet.'\n",
    "        test_caption += ' Abbreviations and key are as in Table~\\\\ref{tab:train_all}.'\n",
    "    else:\n",
    "        train_caption = 'Metrics for the train split of CogNet (language = \\\\texttt{' + lang + '}).'\n",
    "        train_caption += ' Abbreviations and key are as in Table~\\\\ref{tab:train_all}.'\n",
    "        test_caption = 'Metrics for the test split of CogNet (language = \\\\texttt{' + lang + '}).'\n",
    "        test_caption += ' Abbreviations and key are as in Table~\\\\ref{tab:train_all}.'\n",
    "    \n",
    "    with open(train_path, 'w+', encoding='utf-8') as f:\n",
    "        tex = train_df.to_latex(\n",
    "            bold_rows=True,\n",
    "            label=f'tab:train_{lang}',\n",
    "            caption=train_caption,\n",
    "            float_format='%.3f'\n",
    "        )\n",
    "        for name in replace_names:\n",
    "            tex = tex.replace(name, '& \\\\textbf{' + name[2:-1] + '} ')\n",
    "        for num in set(re.findall(floating_point_number, tex)):\n",
    "            tex = tex.replace(num, '\\\\gradient{' + num + '}')\n",
    "        f.write(clean(tex))\n",
    "    with open(test_path, 'w+', encoding='utf-8') as g:\n",
    "        tex = test_df.to_latex(\n",
    "            bold_rows=True,\n",
    "            label=f'tab:test_{lang}',\n",
    "            caption=test_caption,\n",
    "            float_format='%.3f'\n",
    "        )\n",
    "        for name in replace_names:\n",
    "            tex = tex.replace(name, '& \\\\textbf{' + name[2:-1] + '} ')\n",
    "        for num in set(re.findall(floating_point_number, tex)):\n",
    "            tex = tex.replace(num, '\\\\gradient{' + num + '}')\n",
    "        g.write(clean(tex))\n",
    "        \n",
    "    # for measure in ['R@100']:#unagg_results['train']['measure'].unique():\n",
    "    #     subset = unagg_results['train'][unagg_results['train']['measure'] == measure]\n",
    "    #     s = pd.DataFrame(pairwise_tukeyhsd(subset['value'], subset['dist'], alpha=alpha).summary())\n",
    "    #     rej = s[s[6].astype(str) == 'True']\n",
    "    #     if not rej.empty:\n",
    "    #         tex = rej[[0, 1]].to_latex(\n",
    "    #             header=False,\n",
    "    #             index=False,\n",
    "    #             label=f'tab:{measure}_train_{lang}',\n",
    "    #             caption='Significantly different methods for the test split of language \\\\texttt{' + lang + '} according to a TukeyHSD test.'\n",
    "    #         )\n",
    "    #         for name1, name2 in zip(short_names, replace_names):\n",
    "    #             #print(name1, name2.replace('&', '').strip())\n",
    "    #             tex = tex.replace(name1+' ', name2.replace('&', '').strip().replace('\\\\', '')+' ')\n",
    "    #             tex = tex.replace(' '+name1, ' '+name2.replace('&', '').strip().replace('\\\\', ''))\n",
    "    #         with open(os.path.join('sig', f'{measure}_{lang}_train.tex'), 'w+', encoding='utf-8') as h:\n",
    "    #             h.write(clean(tex))\n",
    "                \n",
    "    # for measure in ['R@100']:#unagg_results['test']['measure'].unique():\n",
    "    #     subset = unagg_results['test'][unagg_results['test']['measure'] == measure]\n",
    "    #     s = pd.DataFrame(pairwise_tukeyhsd(subset['value'], subset['dist'], alpha=alpha).summary())\n",
    "    #     rej = s[s[6].astype(str) == 'True']\n",
    "    #     if not rej.empty:\n",
    "    #         tex = rej[[0, 1]].to_latex(\n",
    "    #             header=False,\n",
    "    #             index=False,\n",
    "    #             label=f'tab:{measure}_test_{lang}',\n",
    "    #             caption='Significantly different methods for the test split of language \\\\texttt{' + lang + '} according to a TukeyHSD test.'\n",
    "    #         )\n",
    "    #         for name1, name2 in zip(short_names, replace_names):\n",
    "    #             #print(name1, name2.replace('&', '').strip())\n",
    "    #             tex = tex.replace(name1+' ', name2.replace('&', '').strip().replace('\\\\', '')+' ')\n",
    "    #             tex = tex.replace(' '+name1, ' '+name2.replace('&', '').strip().replace('\\\\', ''))\n",
    "    #         with open(os.path.join('sig', f'{measure}_{lang}_test.tex'), 'w+', encoding='utf-8') as h:\n",
    "    #             h.write(clean(tex))\n",
    "    \n",
    "    measure = 'R@100'\n",
    "    for split in ('train', 'test'):\n",
    "        agg_subset = agg_results[split].loc[measure].rename({\n",
    "            'Cos': 'cos1',\n",
    "            'Dam': 'dam',\n",
    "            'Dice': 'dice1',\n",
    "            'Jac': 'jac1',\n",
    "            'LCS': 'lcs',\n",
    "            'Lev': 'lev',\n",
    "            'Lev$_D$': 'wlev_del',\n",
    "            'Lev$_I$': 'wlev_ins',\n",
    "            'Lev$_S$': 'wlev_sub',\n",
    "            'OSA': 'osa',\n",
    "            'QG1': 'qg1',\n",
    "            'QG2': 'qg2',\n",
    "            'QG3': 'qg3'\n",
    "        })\n",
    "        unagg_subset = unagg_results[split][unagg_results[split]['measure'] == measure]\n",
    "        \n",
    "        # Run Tukey HSD and extract results\n",
    "        tukey_result = pairwise_tukeyhsd(unagg_subset['value'], unagg_subset['dist'], alpha=alpha)\n",
    "        tukey_df = pd.DataFrame(data=tukey_result.summary().data[1:], columns=tukey_result.summary().data[0])\n",
    "\n",
    "        # Build matrix\n",
    "        column_names = sorted(set(tukey_df['group1']) | set(tukey_df['group2']))\n",
    "        sig_matrix = pd.DataFrame('', index=column_names, columns=column_names)\n",
    "\n",
    "        # Fill matrix based on significance and mean comparison\n",
    "        for _, row in tukey_df.iterrows():\n",
    "            g1, g2 = row['group1'], row['group2']\n",
    "            sig = row['reject']\n",
    "            if sig:\n",
    "                if agg_subset.loc[g2] >= agg_subset.loc[g1]:\n",
    "                    sig_matrix.at[g1, g2] = '-'\n",
    "                    sig_matrix.at[g2, g1] = '+'\n",
    "                else:\n",
    "                    sig_matrix.at[g1, g2] = '+'\n",
    "                    sig_matrix.at[g2, g1] = '-'\n",
    "\n",
    "        # Done\n",
    "\n",
    "        # s = pd.DataFrame(pairwise_tukeyhsd(unagg_subset['value'], unagg_subset['dist'], alpha=alpha).summary())[[0, 1, 6]]\n",
    "        # s = s.loc[1:]\n",
    "        # for column in s.columns:\n",
    "        #     s[column] = s[column].astype(str)\n",
    "        # column_names = sorted(list(set(s[0]).union(set(s[1]))))\n",
    "        # sig_matrix = pd.DataFrame(index=column_names, columns=column_names)\n",
    "        # for col in column_names:\n",
    "        #     for row in column_names:\n",
    "        #         if col == row:\n",
    "        #             is_sig = False\n",
    "        #         else:\n",
    "        #             try:\n",
    "        #                 is_sig = ((s.loc[(s[0] == col) & (s[1] == row)])[6] == 'True').values[0]\n",
    "        #             except IndexError:\n",
    "        #                 is_sig = False\n",
    "        #         if is_sig and (agg_subset.loc[row] >= agg_subset.loc[col]):\n",
    "        #             #print(row, col)\n",
    "        #             sig_matrix[col].loc[row] = '+'\n",
    "        #             sig_matrix[row].loc[col] = '-'\n",
    "        #         elif is_sig and (agg_subset.loc[row] < agg_subset.loc[col]):\n",
    "        #             sig_matrix[col].loc[row] = '-'\n",
    "        #             sig_matrix[row].loc[col] = '+'\n",
    "        #         else:\n",
    "        #             sig_matrix[col].loc[row] = ''\n",
    "                    \n",
    "        sig_matrix = sig_matrix.fillna('')\n",
    "\n",
    "        tex = sig_matrix.to_latex(\n",
    "            #bold_rows=True,\n",
    "            label=f'tab:sig_{lang}_{split}_{measure}',\n",
    "            caption='Each row-indexed distance function performs significantly better (worse) than any distance function with a + (-) in its row for language \\\\texttt{' + lang + '}' +\n",
    "            f' {measure} on the {split} split.',\n",
    "            float_format='%.3f'\n",
    "        )\n",
    "\n",
    "        for name1, name2 in zip(short_names, replace_names):\n",
    "            tex = tex.replace(name1, '\\\\textbf{' + name2[2:-1] + '} ')\n",
    "\n",
    "        with open(f'sig/{lang}_{split}_{measure}.tex', 'w+', encoding='utf-8') as file:\n",
    "            file.write(clean(tex).replace(\n",
    "                'w\\\\textbf{Lev} _del', '\\\\textbf{Lev$_D$}'\n",
    "            ).replace(\n",
    "                'w\\\\textbf{Lev} _ins', '\\\\textbf{Lev$_I$}'\n",
    "            ).replace(\n",
    "                'w\\\\textbf{Lev} _sub', '\\\\textbf{Lev$_S$}'\n",
    "            ))\n",
    "        \n",
    "    return agg_results, unagg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in quick_langs:\n",
    "    get_results(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\\\onecolumn')\n",
    "for lang in sorted(quick_langs):\n",
    "    print('\\\\clearpage')\n",
    "    print('\\\\input{tables/' + lang + '_train.tex}')\n",
    "    print('\\\\clearpage')\n",
    "    print('\\\\input{tables/' + lang + '_test.tex}')\n",
    "print('\\\\clearpage')\n",
    "print('\\\\twocolumn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "for file in os.listdir('timing'):\n",
    "    if 'test' in file:\n",
    "        continue\n",
    "    j = json.load(open(f'timing/{file}', 'r', encoding='utf-8'))\n",
    "    btpi = j['tree_build_time_per_item']\n",
    "    tpq = j['query_time_per_query']\n",
    "    if 'wlev' in file:\n",
    "        name = '_'.join(file.split('_')[:2])\n",
    "    else:\n",
    "        name = file.split('_')[0]\n",
    "    d[name] = {\n",
    "        'Index Time Per Item': btpi,\n",
    "        'Time Per Query': tpq\n",
    "    }\n",
    "    \n",
    "df = pd.DataFrame(d).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean(df.to_latex(float_format='%.4f', caption='Timing information.').replace('lrr', '|c|c|c|')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = {}\n",
    "df_train = {}\n",
    "\n",
    "for lang in quick_langs:\n",
    "    df_test[lang] = {}\n",
    "    df_train[lang] = {}\n",
    "    for file in glob.glob(f'metrics/{lang}/*.json'):\n",
    "        basename = os.path.basename(file)\n",
    "        if 'wlev' in basename:\n",
    "            name = '_'.join(basename.split('_')[:2])\n",
    "        else:\n",
    "            name = basename.split('_')[0]\n",
    "        if 'train' in file:\n",
    "            df_train[lang][name] = json.load(open(file, 'r', encoding='utf-8'))['R@100']\n",
    "        else:\n",
    "            assert 'test' in file\n",
    "            df_test[lang][name] = json.load(open(file, 'r', encoding='utf-8'))['R@100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df_train).transpose().rename(columns={\n",
    "    'qg3': '\\\\textbf{QG3}',\n",
    "    'qg2': '\\\\textbf{QG2}',\n",
    "    'qg1': '\\\\textbf{QG1}',\n",
    "    'lcs': '\\\\textbf{LCS}',\n",
    "    'osa': '\\\\textbf{OSA}',\n",
    "    'dice1': '\\\\textbf{Dice}',\n",
    "    'jac1': '\\\\textbf{Jac}',\n",
    "    'cos1': '\\\\textbf{Cos}',\n",
    "    'dam': '\\\\textbf{Dam}',\n",
    "    'lev': '\\\\textbf{Lev}',\n",
    "    'wlev_ins': '\\\\textbf{Lev$_I$}',\n",
    "    'wlev_del': '\\\\textbf{Lev$_D$}',\n",
    "    'wlev_sub': '\\\\textbf{Lev$_S$}'\n",
    "})\n",
    "df_test = pd.DataFrame(df_test).transpose().rename(columns={\n",
    "    'qg3': '\\\\textbf{QG3}',\n",
    "    'qg2': '\\\\textbf{QG2}',\n",
    "    'qg1': '\\\\textbf{QG1}',\n",
    "    'lcs': '\\\\textbf{LCS}',\n",
    "    'osa': '\\\\textbf{OSA}',\n",
    "    'dice1': '\\\\textbf{Dice}',\n",
    "    'jac1': '\\\\textbf{Jac}',\n",
    "    'cos1': '\\\\textbf{Cos}',\n",
    "    'dam': '\\\\textbf{Dam}',\n",
    "    'lev': '\\\\textbf{Lev}',\n",
    "    'wlev_ins': '\\\\textbf{Lev$_I$}',\n",
    "    'wlev_del': '\\\\textbf{Lev$_D$}',\n",
    "    'wlev_sub': '\\\\textbf{Lev$_S$}'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sort_index(axis=1)\n",
    "df_test = df_test.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex = clean(df_train.to_latex(float_format='%.3f'))\n",
    "for num in set(re.findall(floating_point_number, tex)):\n",
    "    tex = tex.replace(num, '\\\\gradient{' + num + '}')\n",
    "print(tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex = clean(df_test.to_latex(float_format='%.3f'))\n",
    "for num in set(re.findall(floating_point_number, tex)):\n",
    "    tex = tex.replace(num, '\\\\gradient{' + num + '}')\n",
    "print(tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in quick_langs:\n",
    "    print('\\\\clearpage')\n",
    "    print('\\\\noindent')\n",
    "    print('\\\\begin{minipage}[t]{0.5\\\\linewidth}')\n",
    "    print('\\\\input{tables/'+lang+'_train.tex}')\n",
    "    print('\\\\end{minipage}%')\n",
    "    print('\\\\begin{minipage}[t]{0.5\\\\linewidth}')\n",
    "    print('\\\\input{tables/'+lang+'_test.tex}')\n",
    "    print('\\\\end{minipage}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
