{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('searches', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cognet\n",
    "cnv2 = pd.read_csv(\n",
    "    'CogNet/CogNet-v2.0.tsv',\n",
    "    sep = '\\t'\n",
    ")\n",
    "cnv2.columns = [\n",
    "    'concept id',\n",
    "    'query_lang', 'word 1',\n",
    "    'doc_lang', 'word 2',\n",
    "    'translit 1', 'translit 2'\n",
    "]\n",
    "\n",
    "# training and test sets already defined\n",
    "training = pd.read_csv(\n",
    "    'CogNet/evaluation/training.tsv',\n",
    "    sep = '\\t'\n",
    ")\n",
    "training.columns = [\n",
    "    'concept id', 'query_lang', 'word_id',\n",
    "    'group_id', 'lemma', 'transliteration'\n",
    "]\n",
    "\n",
    "test = pd.read_csv(\n",
    "    'CogNet/evaluation/test.tsv',\n",
    "    sep = '\\t'\n",
    ")\n",
    "test.columns = [\n",
    "    'concept id', 'query_lang', 'word_id',\n",
    "    'group_id', 'lemma', 'transliteration'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use romanization, fallback to wordform if not provided\n",
    "cnv2['query'] = cnv2['translit 1'].fillna(cnv2['word 1'])\n",
    "cnv2['doc'] = cnv2['translit 2'].fillna(cnv2['word 2'])\n",
    "\n",
    "# same thing, romanization with fallback\n",
    "training['query'] = training['transliteration'].fillna(training['lemma'])\n",
    "\n",
    "test['query'] = test['transliteration'].fillna(test['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "cnv2 = cnv2[['query', 'query_lang', 'doc', 'doc_lang']]\n",
    "\n",
    "training = training[['query', 'query_lang']]\n",
    "\n",
    "test = test[['query', 'query_lang']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean values\n",
    "for column in cnv2.columns:\n",
    "    cnv2[column] = cnv2[column].fillna('')\n",
    "    cnv2[column] = cnv2[column].astype(str)\n",
    "    cnv2[column] = cnv2[column].str.strip()\n",
    "    \n",
    "for column in training.columns:\n",
    "    training[column] = training[column].fillna('')\n",
    "    training[column] = training[column].astype(str)\n",
    "    training[column] = training[column].str.strip()\n",
    "    \n",
    "    test[column] = test[column].fillna('')\n",
    "    test[column] = test[column].astype(str)\n",
    "    test[column] = test[column].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with empty values\n",
    "cnv2 = cnv2[~(cnv2 == '').any(axis=1)]\n",
    "\n",
    "training = training[~(training == '').any(axis=1)]\n",
    "\n",
    "test = test[~(test == '').any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get both directions, let any word be a potential query or doc\n",
    "cnv2_switched = cnv2.copy().rename({\n",
    "    'query': 'doc',\n",
    "    'doc': 'query',\n",
    "    'query_lang': 'doc_lang',\n",
    "    'doc_lang': 'query_lang'\n",
    "})\n",
    "cnv2 = pd.concat([cnv2, cnv2_switched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert unique query-doc pairs, consider both language and word form\n",
    "cnv2 = cnv2.drop_duplicates()\n",
    "\n",
    "training = training.drop_duplicates()\n",
    "\n",
    "test = test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 55456 entries, 578 to 5323296\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   query       55456 non-null  object\n",
      " 1   query_lang  55456 non-null  object\n",
      " 2   doc         55456 non-null  object\n",
      " 3   doc_lang    55456 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "qrels_train = cnv2[cnv2['query'].isin(training['query'])]\n",
    "\n",
    "qrels_test = cnv2[cnv2['query'].isin(test['query'])]\n",
    "\n",
    "qrels_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make query ids for train\n",
    "train_unique_queries = qrels_train['query'].unique()\n",
    "train_query_map = pd.DataFrame({\n",
    "    'query': train_unique_queries,\n",
    "    'query_id': [str(i) for i in range(len(train_unique_queries))]\n",
    "})\n",
    "qrels_train = qrels_train.merge(train_query_map, on='query', how='left')\n",
    "\n",
    "# make query ids for test\n",
    "test_unique_queries = qrels_test['query'].unique()\n",
    "test_query_map = pd.DataFrame({\n",
    "    'query': test_unique_queries,\n",
    "    'query_id': [str(i) for i in range(len(test_unique_queries))]\n",
    "})\n",
    "qrels_test = qrels_test.merge(test_query_map, on='query', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make doc ids\n",
    "unique_docs = cnv2['doc'].unique()\n",
    "doc_map = pd.DataFrame({\n",
    "    'doc': unique_docs,\n",
    "    'doc_id': [str(i) for i in range(len(unique_docs))]\n",
    "})\n",
    "\n",
    "# merge to cnv2\n",
    "old_len = len(cnv2)\n",
    "cnv2 = cnv2.merge(doc_map, on='doc', how='left')\n",
    "assert len(cnv2) == old_len, (len(cnv2), old_len)\n",
    "\n",
    "# add to train qrels\n",
    "old_len = len(qrels_train)\n",
    "qrels_train = qrels_train.merge(doc_map, on='doc', how='left')\n",
    "assert len(qrels_train) == old_len, (len(qrels_train), old_len)\n",
    "\n",
    "# add to test qrels\n",
    "old_len = len(qrels_test)\n",
    "qrels_test = qrels_test.merge(doc_map, on='doc', how='left')\n",
    "assert len(qrels_test) == old_len, (len(qrels_test), old_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevance judgment\n",
    "qrels_train['relevance'] = 1\n",
    "\n",
    "qrels_test['relevance'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort\n",
    "qrels_train = qrels_train.sort_values(\n",
    "    by=['query_id', 'doc_id']\n",
    ")[['query_id', 'query', 'query_lang', 'doc_id', 'doc', 'doc_lang', 'relevance']]\n",
    "\n",
    "qrels_test = qrels_test.sort_values(\n",
    "    by=['query_id', 'doc_id']\n",
    ")[['query_id', 'query', 'query_lang', 'doc_id', 'doc', 'doc_lang', 'relevance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write\n",
    "assert qrels_train.isna().sum().sum() == 0\n",
    "qrels_train.to_json('qrels-train-v2.jsonl', lines=True, orient='records')\n",
    "\n",
    "assert qrels_test.isna().sum().sum() == 0\n",
    "qrels_test.to_json('qrels-test-v2.jsonl', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make queries too\n",
    "queries = qrels_train[['query_id', 'query', 'query_lang']].drop_duplicates(subset='query_id')\n",
    "queries['query_id'] = queries['query_id'].astype(int)\n",
    "assert queries.isna().sum().sum() == 0\n",
    "queries.sort_values(by='query_id').to_csv('queries-train.tsv', sep='\\t', index=False)\n",
    "\n",
    "queries = qrels_test[['query_id', 'query', 'query_lang']].drop_duplicates(subset='query_id')\n",
    "queries['query_id'] = queries['query_id'].astype(int)\n",
    "assert queries.isna().sum().sum() == 0\n",
    "queries.sort_values(by='query_id').to_csv('queries-test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index for bktree to search\n",
    "docs = cnv2[['doc_id', 'doc', 'doc_lang']].drop_duplicates(subset='doc_id')\n",
    "docs['doc_id'] = docs['doc_id'].astype(int)\n",
    "assert docs.isna().sum().sum() == 0\n",
    "docs.sort_values(by='doc_id').to_csv('docs.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
